{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "chunked_sent = conll2000.chunked_sents()[0]\n",
    "print(chunked_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We r using conll2000 corpus to evaluate and train our chunking model. conll2000 corpus contains text data from Wall street journal which r labeled with Part of speech,noun phrase, verb phrase and preposition phrase tags. \n",
    "U can see above in the output, data is in the tree format and below data is in iob format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN', 'B-NP'), ('in', 'IN', 'B-PP'), ('the', 'DT', 'B-NP'), ('pound', 'NN', 'I-NP'), ('is', 'VBZ', 'B-VP'), ('widely', 'RB', 'I-VP'), ('expected', 'VBN', 'I-VP'), ('to', 'TO', 'I-VP'), ('take', 'VB', 'I-VP'), ('another', 'DT', 'B-NP'), ('sharp', 'JJ', 'I-NP'), ('dive', 'NN', 'I-NP'), ('if', 'IN', 'O'), ('trade', 'NN', 'B-NP'), ('figures', 'NNS', 'I-NP'), ('for', 'IN', 'B-PP'), ('September', 'NNP', 'B-NP'), (',', ',', 'O'), ('due', 'JJ', 'O'), ('for', 'IN', 'B-PP'), ('release', 'NN', 'B-NP'), ('tomorrow', 'NN', 'B-NP'), (',', ',', 'O'), ('fail', 'VB', 'B-VP'), ('to', 'TO', 'I-VP'), ('show', 'VB', 'I-VP'), ('a', 'DT', 'B-NP'), ('substantial', 'JJ', 'I-NP'), ('improvement', 'NN', 'I-NP'), ('from', 'IN', 'B-PP'), ('July', 'NNP', 'B-NP'), ('and', 'CC', 'I-NP'), ('August', 'NNP', 'I-NP'), (\"'s\", 'POS', 'B-NP'), ('near-record', 'JJ', 'I-NP'), ('deficits', 'NNS', 'I-NP'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import tree2conlltags, conlltags2tree\n",
    "iob_tagged = tree2conlltags(chunked_sent)\n",
    "print(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166433\n",
      "10948\n"
     ]
    }
   ],
   "source": [
    "print(len(conll2000.chunked_words()))\n",
    "print(len(conll2000.chunked_sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic concept of Trigram based chunking model is chunk is predicted based of specific sequence of 3 parts of speech tags in a row. Let's forst divide data into train and test sets. 90% data for training and remaining for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "shuffled_conll_sents = list(conll2000.chunked_sents())\n",
    "train_set, test_set = train_test_split(shuffled_conll_sents, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now Let's create a chunking model. \n",
    "from nltk import ChunkParserI, TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrigramChunkParser(ChunkParserI):\n",
    "    def __init__(self, train_set):\n",
    " # now will extract only part of speech tags and iob chunked tag pairs.\n",
    "        train_data = [[(pos_tag, chunk_tag) for word, pos_tag, chunk_tag in tree2conlltags(sent)]\n",
    "                      for sent in train_set]\n",
    " # next will train the TrigramTagger\n",
    "        self.tagger = TrigramTagger(train_data)\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for word, pos in sentence]\n",
    " # now will get the chunked tags\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    " # then will assemble the word, part of speech and chunk triplets\n",
    "        conlltags = [(word, pos_tag, chunk_tag)\n",
    "                    for ((word, pos_tag), (pos_tag, chunk_tag)) in zip(sentence, tagged_pos_tags)]\n",
    " # finally will transform the tags into a chunk tree format for our output.\n",
    "        return conlltags2tree(conlltags)\n",
    " # Now we r done building our model. So let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first will train it by passing the training set with TrigramChunkParser\n",
    "trigram_chunker = TrigramChunkParser(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.0%%\n",
      "    Precision:     79.5%%\n",
      "    Recall:        83.1%%\n",
      "    F-Measure:     81.3%%\n"
     ]
    }
   ],
   "source": [
    "print(trigram_chunker.evaluate(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we can see that peformance is not bad for very simple chunk parser. N-gram chunk parsers which we just created are a good start but better performance can b achieved using additional features beyond part of speech tags. A popular high performance example is a classification based model. \n",
    "As u can see the chunking corpus included with NLTK is valuable resource for training specific chunking models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
